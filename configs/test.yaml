defaults:
  - base_ppo
  - _self_

ttrl:
  trees_dir: questions/
  output_dir: datasets/questions
  num_eval: 10
  pass_at_k_params:
    n: 50
    max_new_tokens: 2048
    temperature': 1.0

data:
  reward_fn_key: data_source
  max_prompt_length: 512
  max_response_length: 2048
  train_batch_size: 64
  train_files: Null
  val_files: Null

actor_rollout_ref:
  model:
    path: Qwen/Qwen2.5-7B-Instruct
    enable_gradient_checkpointing: True
    use_remove_padding: True
  actor:
    ppo_mini_batch_size: 64
    ppo_micro_batch_size_per_gpu: 16
    use_kl_loss: False
    kl_loss_type: low_var_kl
    kl_loss_coef: 0.0
    entropy_coeff: 0
    fsdp_config:
      optimizer_offload: False
      param_offload: False
    optim:
      lr: 1e-6
  rollout:
    n: 5
    tensor_model_parallel_size: 2
    name: sglang
    gpu_memory_utilization: 0.6
    log_prob_micro_batch_size_per_gpu: 16
  ref:
    log_prob_micro_batch_size_per_gpu: 16
    fsdp_config:
      param_offload: True

trainer:
  critic_warmup: 0
  n_gpus_per_node: 2
  nnodes: 1
  total_epochs: 1
  logger:
    - console
    # - wandb
  project_name: debug-TTRL
  experiment_name: integration
  test_freq: 10
  val_before_train: False

algorithm:
  use_kl_in_reward: False
  adv_estimator: grpo

tufa_custom:
  val_logger:
    logs_dir: /root/logs_dir
